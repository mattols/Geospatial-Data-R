---
title: "Task 7"
author: "GEOG-490R"
date: "Spring 2024"
output: 
  html_document:
    code_folding: hide
---

```{r echo=FALSE, message=FALSE}
library(dplyr);library(ggplot2)
```

### Geospatial Data

Welcome to the world of geospatial data! This is the focus of this course. Spatial data could be thought of any information with a spatial (x, y) component, though it typically refers to data with location information. Geospatial data is unique in that the spatial information pertains to location information on the Earth. 

While a simple Cartesian coordinate systems is used to display data on a x/y plane, a geographic coordinate system (GCS) uses other coordinates (e.g., long/lat, easting/northing) to represent data on a geographic system. Several reference systems exist using different estimations (datums) of the real world. Furthermore, data can be projected to to transform 3D GCS information into a 2D plane, which is essential for maps. Keep in mind that projections introduce distortions of area, size, and shape into spatial visualization and analysis. Chosen projections should be aware of location, scale, and situation for any analysis.

Over the next two weeks, be sure to read through the [spatial data with terra section](https://rspatial.org/spatial/2-spatialdata.html) in the book. These sections will familiarize you with vector/raster model structures, GCS/Projections, and the basic spatial data workflows and tools.

This week, you will specifically focus on **vector data models.**

\

#### 1. Basics of spatial data

Geospatial data represents real-world features. Discrete objects are typically represented as points, lines, or polygons. Recall our demonstration from class, where we simulated precipitation data for weather stations along a series of coordinates labeled as longitude and latitude. 

```{r, echo=FALSE}
# Simple representation of spatial point data
name <- LETTERS[1:10]
longitude <- c(-116.7, -120.4, -116.7, -113.5, -115.5,
               -120.8, -119.5, -113.7, -113.7, -110.7)
latitude <- c(45.3, 42.6, 38.9, 42.1, 35.7, 38.9,
              36.2, 39, 41.6, 36.9)
stations <- cbind(longitude, latitude)
set.seed(0)
precip <- round((runif(length(latitude))*10)^3) # Simulated rainfall data
# Plot
psize <- 1 + precip/500
plot(stations, cex=psize, pch=20, col='red', main='Precipitation')
text(stations, name, pos=4)
# add a legend
breaks <- c(100, 250, 500, 1000)
legend.psize <- 1+breaks/500
legend("topright", legend=breaks, pch=20, pt.cex=legend.psize, col='red', bg='gray')
```

Point data is the most basic form of a vector data model. Line and polygon data models can also be incredibly useful. In the most basic sense, these data formats rely on either knowing the correct drawing order of points (line) or start and end with the same point (polygon), closing the feature.

While the plot above could very well represent true geospatial data, there is no defined CRS or projection. The data above is really just a Cartesian coordinate system, where we have labeled x/y as longitude/latitude.There are additional rules and conventions to follow in geospatial data formats, which you will learn more about over time. Geospatial data must contain several types of information, ideally all within a single callable object in R. Fortunately, there are several useful packages dedicated to working with geospatial data formats.

\

#### 2. Vector Data Models

As mentioned before, vector data typically represents discrete objects as a series of points, lines, or polygons. These objects contain various attributes about the surface. For example, imagine a stream polyline dataset. Each stream tributary could contain information about the stream gradient, monthly discharge, average width, average depth, gaining/losing discharge characteristics, water quality information, as well as several other attributes. 

We will want a data model that can store the geometry of the data, detail the spatial reference information, and contain variaous attributes about the features all in one data structure. This is when we need a package that specializes in spatial data.

You can create a vector object with the `vect()` function in the `terra` package.

```{r, message = FALSE, class.source = 'fold-show'}
library(terra)
longitude <- c(-116.7, -120.4, -116.7, -113.5, -115.5, -120.8, -119.5, -113.7, -113.7, -110.7)
latitude <- c(45.3, 42.6, 38.9, 42.1, 35.7, 38.9, 36.2, 39, 41.6, 36.9)
lonlat <- cbind(longitude, latitude)
pts <- vect(lonlat) # create vector data from long/lat points
class(pts)
pts # data structure
```

This data does not have any spatial reference information or attribute information. At the moment it is just a structure containing coordinates. Let's define the CRS for this data. Fortunately, we know that GPS measurements are typically recorded in the standard WGS 1984 datum.

```{r, class.source = 'fold-show'}
crdref <- "+proj=longlat +datum=WGS84" # CRS used for GPS data
pts <- vect(lonlat, crs=crdref)
cat(crs(pts))
```
You can also use EPSG codes, which are spatial reference identifiers. EPSG stands for the European Petroleum Survey Group, but they are widely used in geospatial data analysis. Rather than the PROJ4 (library) script above, you could use: `"EPSG:4326"`, which is the code for the WGS 1984 datum.

Now we can generate random precipitation data just like before. The `runif()` creates a random sample of values within a normal distribution between defined min/max values. We can then add this data to a dataframe and input the point attribute information into the spatial vector object: 

```{r, class.source = 'fold-show'}
# Generate random precipitation values between 0-100, same quantity as points
precipvalue <- runif(nrow(lonlat), min=0, max=100)
df <- data.frame(ID=1:nrow(lonlat), precip=precipvalue)
# combine spatvect with df
ptv <- vect(lonlat, atts=df, crs=crdref)
ptv
```

We can also generate simple line and polygon features in a similar method:

```{r}
## Lines and Polygons
lon <- c(-116.8, -114.2, -112.9, -111.9, -114.2, -115.4, -117.7)
lat <- c(41.3, 42.9, 42.4, 39.8, 37.6, 38.3, 37.6)
lonlat <- cbind(id=1, part=1, lon, lat)
lonlat
# create lines/polgon objects
pols <- vect(lonlat, type="polygons", crs=crdref)
# plot
plot(pols, las=1)
plot(pols, border='black', col='deepskyblue3', lwd=3, add=TRUE)
points(pts, col='firebrick', pch=20, cex=3)
```

Note that the polygon is a single-part polygon feature. A polygon or line feature may relate to a single feature in the real world. As such, you may want a vector model that contains a multi-part lines/polygon feature, which is very common. At this point, we probably don't want to continue to create these objects from scratch. You can, but you will mostly be working with existing spatial data. At most, you will find your self converting point data from a csv table of long/lat points. This is the workflow for incorporating basic field measurements into GIS.

\

#### 3. Existing data

At this point in your GIS journey, you should be familiar with various different vector model data formats. I imagine you are mostly familiar with shapefiles (.shp), which is the ESRI standard vector format. Several other vector data models also exist! Most of these formats can be read by the `terra` package.

terra has a few built-in datasets, let's look at the the Luxembourg dataset:

```{r, class.source = 'fold-show'}
f <- system.file("ex/lux.shp", package="terra")
lux <- vect(f)
lux
```

You can plot attributes from this spatial data by calling the variables fo interest:

```{r}
plot(lux, "NAME_2")
```

You can also extract or add attribute information:

```{r, class.source = 'fold-show', eval=F}
lux$NAME_2 # extract variable values
lux[, "NAME_2"] # extract variable with geometry
lux$lets <- sample(letters, nrow(lux)) # add new or edit variable
lux$lets <- NULL # remove variable
```

In may ways, you can treat geospatial vector data models like a dataframe, both contain attributes that can be analyzed, plotted, joined, or subset. We will learn about specific vector analysis and overlay operations in the coming weeks.

Here are a few final map visualization tips using the `terra::plot()` function:

```{r, warnings=F, message=F}
library(maps)
par(mfrow=c(1,2)) # change plot frame
# map 1
map("world", c("Belgium", "Germany", "Luxembourg"), ylim=c(48,53), xlim=c(3,12), mar=c(1,1,3,1), col='grey30', bg="grey", lwd=3)
title("Study Region Location"); box()
plot(ext(lux), border="firebrick", lwd=1.5, add=T)
map.text("world", c("Belgium", "Germany$"), add=T)
# map 2
plot(lux, "POP", col=blues9, main = "Cantons of Luxembourg", mar=c(2,1,2,2), type="interval", legend="topright", plg=list(title="Population", cex=0.5, bty="o"))
map("world", c("Belgium", "Germany", "Luxembourg"), col='grey40', lty=3, lwd=2, add=T)
map.text("world", "Luxembourg", add=T, col="grey30", cex=0.8)
text(x=6.4, y=49.95, "Germany", cex=0.8, col='grey30')
```

To interpret this:

* the `par()` function allows you to set graphical parameters, the mfrow argument sets the number of c(row, column) plots
* the `mar` argument within the plot functions accesses the `par(mar=...)` parameter, which sets the plot margins
* `box()` draws the black rectangular plot box, `axis`, `text`, and `title` components can also be added, including text from spatial objects and from graphical parameters (see last two lines)
* the `maps` library can be used to quickly visualize country and state borders alongside other spatial data (including text). Note that the map scale at which these are generated is insufficient for precision (see right plot above)
* use `?plot` (from terra) to learn more about the different parameters; common arguments include line width (`lwd=`), line type (`lty=`), size (`cex=`), `plg` to access plot legend parameters, and `add=T` to continue adding data to the current plot
* within map plots you can include a `type=` argument to specify whether to plot your values as "continuous", "classes", or "interval" - the default will be chosen based on your data type (i.e. numeric vs. factor)
* note that legend arguments can be given within the plot function as well as in a new `legend()` function on the next line.

Don't feel the need to spend too much time trying to make a beautiful maps yet! You'll learn more as you go and discover other workflows and packages for plotting. 

\

#### Your Task

Your task this week will explore a few basics related to spatial data. **Only use the** `terra` **package** for your task this week. There are other geospatial libraries that we will use eventually.

\

#### 1. Basic Spatial Data

Read in the "UTSNTL_META.csv" file. This table lists basic meta data for each snotel site throughout the state, specifically, the coordinate location, site name and elevation. 

1. Only using the `terra` package and `vect()` function, transform this dataframe into a spatial vector data object and create a basic plot showing the snotel stations colored based on different elevation zones. 

2. (2 points) Next, you will need to revisit your code for task 6. Summarize the "UTSNTL_ALL_2020_2022_Daily_Wide.csv" table to only show the max snow depth and station name for each station during the 2021-22 winter season. Create a join between your summarized max snow data and the table containing longitude, latitude, and elevation. Be sure that the join is completely successful! (i.e. check that there are no NA values in the joined fields) Make this into a SpatialVector object. Create a basic spatial plot (with terra `plot`) showing max snow depth as a *continuous* variable. Add the outline of the state of Utah using the maps package.

**Tip:** *try on your own first!*
```{r}
# the maps package can use the argument
# map(database="state", ...)
# rather than "world", to find u.s. state data
# ?map to learn more
```

You've learned in your GIS classes that data classification can enhance, or mislead, map interpretation. If you need a reminder of different data classification methods for choropleth maps, read [this gisgeography article](https://gisgeography.com/choropleth-maps-data-classification/).
You will be comparing a few data classification methods in the next question.

*Note, a great resource to review color options is the [R color cheatsheet](https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf).*

3. (3 points) Using the terra plot functions, create three plots side-by-side showing the max snow depth at snotel sites in Utah during the 2021-22 season. For methods, use equal interval with 7 classes, quantile, and standard deviation (review link above if needed). Make the classification method the title of the legend in each and use sequential color schemes (one color) for equal interval and quantile, and a diverging color scheme (two colors) for the standard deviation classification method (see link above for more). Note the standard deviation scheme should all include 1-3 standard deviations from the mean ($\mu \pm 3\sigma$). Add the outline of the state with the maps package for each map. Provide 2-3 sentences describing the differences, benefits, and meaning of the classification methods used.

**Tips:** *try on your own first!*
```{r}
# use the par() mafrow argument to generate the proper number of plots, may need to adjust the mar function as well.
# all of these classifications will be type="interval"

# there are several stats functions in base R for generating the breaks from quantile and standard deviation distributions
# intervals are defined by the breaks= argument in the plat (which can list a number or the actual breaks in the data classification)
# one option to generate a sequential color schemes is:
# colorspace::sequential_hcl(5)
# read the Rcolorcheatsheet link above for more
```

\

#### 2. Manipulating Vector Data

Read in the "UT_HUC8.shp" file as a SpatialVector object with `terra`. This multipart polygon feature shows the various hydrologic units (watersheds) throughout the state of Utah.

4. Create a basic map plot in terra that shows the individual watersheds colored by their unique names. Rather than adding a legend, use the `text()` function with the spatial data to add the names of each watershed to the map.

5. Create an extent object (rectangular box) using the `ext()` function in terra for Utah Valley using the following: Latitude range: 39.915063 to 40.471646; Longitude range: -111.952211 to -111.536395. Plot the geometry of the UT HUC layer and add the rectangular bounding box to the map in red, showing this as an area of interest. Add a legend in the top right of the map indicating what the area represents.

**Tips:** *try on your own first!*
```{r}
# look at see how to input the values above into an ext() object
# there are two ways to approach this:
# 1. Note that ext() objects do not have a CRS. You can re-project (project() function) your data into a lat/long CRS to add the extent object to the map.
# 2. Alternatively, and probably better practice, you can convert the extent object to a polygon with the as.polygons() function, add the CRS for the basic GPS long/lat datum, then project the polygon to the CRS of the HUC data.
```

6. Use the same workflow you used in the previous question along with the `crop()` function to clip the UT HUC watersheds data to only show the part of the state above 39 degrees Latitude.

Run the following code to use online vector data for Antelope Island:

```{r, eval=FALSE, class.source = 'fold-show'}
island <- vect("https://raw.githubusercontent.com/mattols/geospat_data/main/AntelopeIsland.geojson")
island
```

A geojson file is similar to a shapefile, just another vector model format. We can read in this file as vector data from an online source.

7. Create a subset of the UT_HUC vector that only shows the Great Salt Lake watershed and plot the results. Then use the island vector above to subtract the island out of the GSL watershed, plot this in a color that might be used for water on a map. Add a text label to this plot indicating the location of Antelope Island.

**Tips:** *try on your own first!*
```{r}
# Use indexing and boolean logic to subset the GSL HUC
# you may need to re-project the island vector to match that of the HUC data
# the erase() function can be used to subtract area from a polygon
```

The hydrologic unit code (HUC) listed in this dataset represents a level of watershed in the USGS watershed boundary dataset. There are anywhere from 2 to 12 hydrologic units that exist, which relate to large-scale (2) and small (12) watersheds throughout the US. The HUC code listed in this dataset is HUC8, as it contains 8 digits. Visit the USGS site to learn more about the [watershed boundary dataset](https://www.usgs.gov/national-hydrography/watershed-boundary-dataset). While you cannot extrapolate any smaller than the HUC-8 level, the code contains information about larger watersheds. For example, the first two digits in the HUC code correspond to the HUC2 watershed. You will dissolve the HUC8 data into a HUC4 vector model.

8. (3 points) Create a new variables for the HUC4 and HUC2 codes, based on the HUC8 code. Dissolve the spatial vector into HUC4 units, then again in HUC2 units. Finally, create a map plot showing all HUC8 watersheds (line with color fill), along with HUC4 and HUC2 watersheds overlayed (lines no color). Add text for each HUC4 unit. Use color, linewidth, linetype, and other arguments to distinguish between the different watershed units in the map plot.

**Tips:** *try on your own first!*
```{r}
# the substr() function can be helpful for splitting apart strings
# Use the aggregate() function in terra to dissolve vector data based on an attribute
```

\

#### 3. Spatial Joining

Spatial data starts to become very powerful when we can join information together spatially. Consider, we have attribute information from our snotel stations, and we have watershed information. Let's add the two together

9. (3 points) Use either the `intersect` or `extract` function in terra to obtain the HUC code for each snotel station. The result should be either a dataframe vector model of the same length as snotel sites. Investigate the affect of elevation on max snow by HUC8 code (you can filter any values that are 0). Recall, that you did this in T06, but now can assess this relationship by individual watersheds. Discuss what you find and how these results differ from the relationship of max snow depth ~ elevation.

**Tips:** *try on your own first!*

```{r}
# you can gain a general sense of correlation with a scatterplot of your variables, in this case you would want to facet the data by HUC code
# To statistically assess the relationship run the lm() function. 
```

10. (4 points) Summarize the table values you made in the previous question to calculate the max snow depth, mean elevation, and number of stations for each HUC8. Join this to your UT HUC vector layer and create a final map plot showing two maps side-by-side: 1) showing HUC8 colored by max snow depth with the number of stations labeled as text, 2) showing HUC8 colored by mean elevation with max snow depth added as text. Consider good choices of classification method, color, and other attributes to enhance these maps.

**Tips:** *try on your own first!*
```{r}
# summarize with dplyr, where you will group by the HUC units and use the summarise function
# n() is a nifty function to calculate the total number or count of variables
# the easiest way to join the data might be by adding a new variable to UT HUC vector object using the $ operator, match() with HUC code, and indexing, however, there are many other ways
# review the workflow for plotting and adding text and plot parameters above.
```

\

#### Submitting

Upload your R Markdown output as an html file to Canvas

\

**This assignment is due in one week but may be turned for an additional week with a late penalty of -5%**

\
