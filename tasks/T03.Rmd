---
title: "Task 3"
author: "GEOG-490R"
date: "Spring 2024"
output: html_document
---

### Data Formatting and Simple Statistics

#### 1. Additional Data Formats

R provides specialized data structures beyond regular numeric vectors and data frames for representing certain types of data more effectively. For dates and times, R has Date, POSIXct, and POSIXlt objects. These store temporal data in ways that allow easy extraction of components like year, month, day, hour, minutes, etc using built-in functions. 

```r
now <- Sys.time() # current date and time
class(now)
#[1] "POSIXct" "POSIXt"
```

POSIXct stores times as the number of seconds since the Epoch, while POSIXlt uses a list structure with the components broken out. Converting between the two is straightforward using as.POSIXlt() and as.POSIXct().

We can easily extract components like year, month, day, hour, minutes, etc using built-in functions. Converting between formats is also simple:

```r
now2 <- as.Date(now) # convert to Date class
as.POSIXlt(now2) # back to POSIXlt
```
There are several ways to format and change date/time information. It can be a headache but is essential for some analysis.

```{r}
today <- Sys.Date()
format(today, "%d %b %Y")  # with month as a word
```

Make sure your dates are formatted correctly, in some instances a correct timezone may even need to be specified. 

For dictionary-like data, R has lists which contain key-value pairs instead of just values like regular vectors. Lists are created with the list() function and elements are accessed using the $ operator. This makes them useful for storing parameters, configurations, and custom data structures. 


We discussed lists previously, they are similar (but different) from dictionaries, which are commonly used in languages like python. We can create a named lists using the list() function:

```r
params <- list(mean=0, sd=1) 
params$mean # extract an element by name
```
Dictionaries are not always the most useful way to store data but can come in handy. Some model outputs may be stored in dictionary-like lists.

As discussed earlier, the NA value represents missing data. is.na() tests for NA values:

```r
vals <- c(1, 2, NA, 4)
is.na(vals) # [1] FALSE FALSE TRUE FALSE
```

However, NA values propagate through operations, and must consequently be dealt with. Two options that exist include omitting NA values or ignoring them within the function:

```r 
vals * 2 # [1]  2  4 NA  8
mean(vals) # NaN 
na.omit(vals) # [1] 1 2 4
mean(vals, na.rm = TRUE) # [1] 2.333333
```

We'll discuss one other option for dealing with NA values later on.


#### 2. Indexing, Filtering, and Transforming DataFrames in Base R

R's built-in data frame manipulation functions allow much processing and analysis without needing additional packages. Rows and columns can be extracted using integer indexing or names. Logical subsetting with a vector corresponding to TRUE values elegantly filters data frames based on conditions.

Brackets `[ ]` are used to index objects:

```r
df[1, 2]
df[1:5, c("col1", "col2")] # first 5 rows, two columns
df[df$col1 > 0, ] # logical subset rows where col1 is positive
```
Indexing and subsetting can be performed when:
- Location is specifid - df[i, j] where i are rows, j are cols
- Subsets are sliced df[1:5, ]
- Specific cols df[, c("col1", "col2")]
- Logical conditions are met

You can also slice a subset of a vector of dataframe

```r
v <- seq(100,120)
v[10:15] # [1] 109 110 111 112 113 114
```

Full rows, columns, and other DataFrames can also be added or removed with cbind(), rbind(), and subset(). 

New columns can be added simply by assigning to the $ operator.

```r
df$new_col <- df$col1 * 100 # add new column
```

Sorting is done via the order() and sort() functions.

```r
sort(df$col1) # sort values
df <- df[order(df$col1),] # reorder data based on order of col1
```

This brings us to another method to handle NA values:

```r
vals <- c(1, 2, NA, 4)
vals[is.na(vals)] <- 0 
```

When working with large datasets, you will likely need to subset and manipulate the original data into a format that is better for your analysis. The base functions in R have many useful capabilities, however, you will soon be introduced to other libraries in R that can perform these same functions and more.


#### 3. Reading and Writing Files & Filepaths

Importing and exporting data is essential in any analysis workflow. Tabular data from CSV files can be read into R data frames using the read.csv() and read.table() functions. 

*What is a filepath?* 
Hopefully you already know a bit about filepaths, but let's assume someone in class needs a reminder. 

Filepaths specify the location of a file or directory (folder). Think about this like your Documents folder on your computer. The full filepath to your documents folder might be specified as "~/Documents" or may be "C:\Users\12345678\Documents" depending on your setup.

A working directory is where all commands are being executed on your computer, R will open with the default working directory specified. Use `getwd()` to list the path of the working directory. R's working directory is changed with `setwd()`. We can see the current files/folders using a few functions:

```r
setwd("../data") # set working directory
dir() # list files in working directory
list.dir() # list all directories and sub-folders
```

The Tab key can be useful for entering a filepath. Keep in mind that the path should be a character, specified with " ". Filepaths starting with "/" are absolute, while "data/file.csv" is relative within the working directory.

Once, you have the filepath of a known file. You can use functions to import the data into R. The most simple form of tabular data is a csv file. While you can use other packages to read in files like an Excel file (.xlsx), I think it is generally best to stick to a csv file for less headache. 

Functions like `read.csv()` and `read.table()` import tabular data files into R data frames. These files can then be manipulated and exported (saved) as a physical file on your computer:

```r
df = read.csv("data.csv") # load from csv
# steps to clean df
write.csv(df, "clean_data.csv") # export CSV 
```

There are several other packages that facilitate reading some forms of data. Geospatial data, in particular, requires a specialized library to interpret in R.

Data frames can be saved to disk using write.csv() and write.table(). This facilitates realistic data analysis pipelines entirely within R: read in data, wrangle, process, analyze, and export results.

A good segue into the next section is considering how we might easily find a file, or several types of files within the folders of our computer. We can use pattern matching or regular expression.

For example, consider that we have a very unorganized folder but we know that there are one or more .csv files. When listing files, you can add a pattern argument to the function so that the results only lists the type of files you are looking for:

```r
filename <- list.files("data/unorganized_folder", pattern = ".csv") 
df <- rea.csv(filename)
```

Read more about regular expression in the next section!

#### 4. Regular Expression

Characters, also known as "string" are a primary data type in any programming language. Regular expressions provides powerful string pattern matching capabilities using special characters.  This makes regular expressions invaluable for processing text-based data.

The first and easiest way to parse a character string is by splitting the string based on a known character.

```r
strsplit("Jose_Gonzalez", "_") # [1] "Jose"     "Gonzalez"
```

Other functions like `grep` and `grepl` search for matches to argument pattern within each element of a character and differ in the format of and amount of detail in the results. Whereas functions like `sub` and `gsub` perform replacement of the first and all matches respectively.

```r
grep("\\d", c("abc123", "def")) # match strings containing digits
grepl("[A-Z]", c("abc", "XYZ")) # test for uppercase 
gsub("-", "_", c("text-data", "more-text")) # replace - with _
gsub("[[:punct:]]", "", c("Hi there!", "How are you?")) # remove punctuation
```
Regular expressions provide a powerful pattern matching syntax for text data. While you likely will not likely become an expert in regex, it is useful to know some if you want to automate some of the tedious tasks.

Some useful special characters for matching patterns include:

- \\d - digits
- \\s - whitespace
- . - any character
- [] - set/range of characters *(e.g. [0-9] are digits and [a-z] are lowercase letters)*
- ^ and $ - start and end of string

By combining these special characters with repetition qualifiers like * and +, we can create complex regex patterns to match all sorts of text-based formats. Here's a [cheatsheet](https://hypebright.nl/index.php/en/2020/05/25/ultimate-cheatsheet-for-regex-in-r-2/) if you want to know more.

#### 5. Basic Models

**Linear Models**

- Fit linear regression models with the lm() function
- Specify formula like y ~ x1 + x2
- summary() shows model coefficients, p-values
- fitted() and residuals() extract fitted values and residuals
- confint() gives confidence intervals for coefficients
- Should check model diagnostics like residual plots  

```r
fit <- lm(y ~ x1 + x2, data=mydata)
summary(fit) 
```
<><><>

R comes equipped out of the box with all of the standard statistical tests and models used in data analysis. Linear models can be fit using the `lm()` function and analyzed with `summary()`, `confint()`, and extractor functions for residuals, predictions, etc. 

We can fit simple linear regression models in R with the lm() function. It takes a formula as input describing the relationship between variables:

```r 
fit <- lm(y ~ x1 + x2, data=mydata)
summary(fit) # view model summary
confint(fit) # 95% CIs for coefficients 
```

Model residuals, predictions, diagnostics, and additional details can be extracted with extractor functions. We should check residuals for normality and homoscedasticity.

**significance**

Statistical tests like linear models produce p-values to test null hypotheses. P-values below 0.05 are often considered statistically significant:

```r
coef(summary(fit)) # view p-values for each coefficient
```

Get back t-statistic, degrees of freedom, p-value

However, with many comparisons, false positives become likely by chance (multiple comparisons problem). Adjustments like Bonferroni correction may be necessary.

- fitted() and residuals() extract fitted values and residuals

**ANOVA and T-tests** 

Hypothesis tests like t-tests, ANOVA, correlation tests are carried out with t.test(), aov(), and cor.test().

For group differences, R provides 1 and 2 sample t-tests, as well as ANOVA for factor-based group comparisons:

```r
t.test(x, y) # two sample t-test

aov(y ~ groups, data=mydata) # one-way ANOVA 
```

These tests make assumptions like normality and equal variance that should be checked. If violated, non-parametric tests may be better.

#### 4.4 Chi-squared and Correlation

Chi-squared tests allow analyzing relationships between categorical variables. Correlation like Pearson's r characterize numeric associations: 

```r
chisq.test(table(x, y)) 

cor.test(x, y) # correlation test 
```

Pearson for linear, Spearman for monotonic relationships

Scatterplots also help visualize correlations.


<><>



**t-tests** 

- Compare two groups with t.test()
- Specify the two samples with formula like x ~ y
- Get back t-statistic, degrees of freedom, p-value
- Paired (repeated measures) or unpaired (independent samples)

```r
t.test(x, y)
```

**ANOVA**

- Compare more than two groups with one-way ANOVA 
- aov(y ~ groups, data=mydata)
- Get F-statistic, p-value 
- Determine which groups differ using post-hoc tests like Tukey's 

```r
aov(y ~ groups, data=mydata)
```

**Correlation** 

- Test correlation between two numeric variables with cor.test()
- Get correlation coefficient and p-value
- Visualize with scatterplot and fitted line
- Pearson for linear, Spearman for monotonic relationships   

```r  
cor.test(x, y) 
```



<><>


### Your Task 

Answer the following 10 questions based on this week's content by writing an annotated R script.

1. Load the mtcars dataset and calculate the correlation between mpg and wt. Is this correlation statistically significant at the 0.05 level?

2. For the mtcars data, convert the vs variable from numeric to a factor. How many levels does this factor have? 

3. Write a regular expression that would match valid email addresses containing exactly one @ symbol and at least one period.

4. Load the iris dataset and create a new column called sepal_area by multiplying sepal_length by sepal_width. 

5. Write a function that calculates the mean and standard deviation of a numeric vector passed as input. Have it return a named list with two elements.

6. Using the mtcars data, test whether the average mpg is different between cars with 4, 6, and 8 cylinders using ANOVA.

7. Extract the year, month, and day components from the following date: "2023-03-14"

8. List only the csv files in the current working directory.

9. Read the mtcars CSV file from Github into R (url: https://raw.githubusercontent.com/plotly/datasets/master/mtcars.csv)

10. Write a function that simulates 10 coin flips and returns the number of heads. Run this simulation 1000 times and plot the distribution of heads.



<><><><><><><><><><><><><>






## Data Formatting and Basic Statistics

### 1. Additional Data Formats

#### Time Formats

#### Dictionary-like Lists

#### Additional 

### 2. Regular Expression

### 3. Indexing and Editing DataFrames (base R)

### 4. Basic Models

#### 4.1 Statistical Linear Models

#### 4.2 Significance

#### 4.3 ANOVA and T-tests

#### 4.4 Chi-squared and Correlation

### 5. Reading and Writing Files & Filepaths

#### 5.1 Filepaths

#### 5.2 Loading and Saving Real Data

### Your Task

Answer the following 10 question based on this week's content by writing an annotated R script.
