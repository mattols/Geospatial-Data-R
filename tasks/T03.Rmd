---
title: "Task 3"
author: "GEOG-490R"
date: "Spring 2024"
output: html_document
---

### Data Formatting and Statistics

#### 1. Additional Data Formats

R provides specialized data structures beyond regular numeric vectors and data frames for representing certain types of data more effectively. For dates and times, R has Date, POSIXct, and POSIXlt objects. These store temporal data in ways that allow easy extraction of components like year, month, day, hour, minutes, etc using built-in functions. 

```r
now <- Sys.time() # current date and time
class(now)
#[1] "POSIXct" "POSIXt"
```

POSIXct stores times as the number of seconds since the Epoch, while POSIXlt uses a list structure with the components broken out. Converting between the two is straightforward using as.POSIXlt() and as.POSIXct().

We can easily extract components like year, month, day, hour, minutes, etc using built-in functions. Converting between formats is also simple:

```r
now2 <- as.Date(now) # convert to Date class
as.POSIXlt(now2) # back to POSIXlt
as.Date("1969-08-15") # convert string to date (tries format "%Y-%m-%d" by default)
```
There are several ways to format and change date/time information. It can be a headache but is essential for some analysis.

```{r}
today <- Sys.Date()
format(today, "%d %b %Y")  # with month as a word
```

The format function can transform your date and time information, however, there are also other helpful functions to do so:

```r
now <- Sys.time() # current date and time
format(now, "%H") # extract hour of time stamp
weekdays(now) # list the day of the week
```

Make sure your dates are formatted correctly, in some instances a correct timezone may even need to be specified. Time is an essential component in analysis, make sure that you understand how to work with dates and time. 

For dictionary-like data, R has lists which contain key-value pairs instead of just values like regular vectors. Lists are created with the list() function and elements are accessed using the $ operator. This makes them useful for storing parameters, configurations, and custom data structures. 


We discussed lists previously, they are similar (but different) from dictionaries, which are commonly used in languages like python. We can create a named lists using the list() function:

```r
params <- list(mean=0, sd=1) 
params$mean # extract an element by name
```
Dictionaries are not always the most useful way to store data but can come in handy. Some model outputs may be stored in dictionary-like lists.

As discussed earlier, the NA value represents missing data. is.na() tests for NA values:

```r
vals <- c(1, 2, NA, 4)
is.na(vals) # [1] FALSE FALSE TRUE FALSE
```

However, NA values propagate through operations, and must consequently be dealt with. Two options that exist include omitting NA values or ignoring them within the function:

```r 
vals * 2 # [1]  2  4 NA  8
mean(vals) # NaN 
na.omit(vals) # [1] 1 2 4
mean(vals, na.rm = TRUE) # [1] 2.333333
```

We'll discuss one other option for dealing with NA values later on.


#### 2. Indexing, Filtering, and Transforming DataFrames in Base R

R's built-in data frame manipulation functions allow much processing and analysis without needing additional packages. Rows and columns can be extracted using integer indexing or names. Logical subsetting with a vector corresponding to TRUE values elegantly filters data frames based on conditions.

Brackets `[ ]` are used to index objects:

```r
df[1, 2]
df[1:5, c("col1", "col2")] # first 5 rows, two columns
df[df$col1 > 0, ] # logical subset rows where col1 is positive
```
Indexing and subsetting can be performed when:
- Location is specifid - df[i, j] where i are rows, j are cols
- Subsets are sliced df[1:5, ]
- Specific cols df[, c("col1", "col2")]
- Logical conditions are met

You can also slice a subset of a vector of DataFrame

```r
v <- seq(100,120)
v[10:15] # [1] 109 110 111 112 113 114
```

Indexing can be very useful to subset data, either by providing the position of the elements you wish to extract `[numbers_to_keep]` or by giving a boolean vector of the same length indicating which `TRUE` values to keep. 

Full rows, columns, and other DataFrames can also be added or removed with cbind(), rbind(), and subset(). New columns can be added simply by assigning to the $ operator.

```r
df$new_col <- df$col1 * 100 # add new column
```

Sorting is done via the order() and sort() functions.

```r
sort(df$col1) # sort values
df <- df[order(df$col1),] # reorder data based on order of col1
```

This brings us to another method to handle NA values, which may be preferrable based on the situation:

```r
vals <- c(1, 2, NA, 4)
vals[is.na(vals)] <- 0 
```

When working with large datasets, you will likely need to subset and manipulate the original data into a format that is better for your analysis. The base functions in R have many useful capabilities, however, you will soon be introduced to other libraries in R that can perform these same functions and more.


#### 3. Reading and Writing Files & Filepaths

Importing and exporting data is essential in any analysis workflow. Tabular data from CSV files can be read into R data frames using the read.csv() and read.table() functions. 

*What is a filepath?* 
Hopefully you already know a bit about filepaths, but let's assume someone in class needs a reminder. 

Filepaths specify the location of a file or directory (folder). Think about this like your Documents folder on your computer. The full filepath to your documents folder might be specified as "~/Documents" or may be "C:\Users\12345678\Documents" depending on your setup.

A working directory is where all commands are being executed on your computer, R will open with the default working directory specified. Use `getwd()` to list the path of the working directory. R's working directory is changed with `setwd()`. We can see the current files/folders using a few functions:

```r
setwd("../data") # set working directory
dir() # list files in working directory
list.dir() # list all directories and sub-folders
```

The Tab key can be useful for entering a filepath. Keep in mind that the path should be a character, specified with " ". Filepaths starting with "/" are absolute, while "data/file.csv" is relative within the working directory.

Once, you have the filepath of a known file. You can use functions to import the data into R. The most simple form of tabular data is a csv file. While you can use other packages to read in files like an Excel file (.xlsx), I think it is generally best to stick to a csv file for less headache. 

Functions like `read.csv()` and `read.table()` import tabular data files into R data frames. These files can then be manipulated and exported (saved) as a physical file on your computer:

```r
df = read.csv("data.csv") # load from csv
# steps to clean df
write.csv(df, "clean_data.csv") # export CSV 
```

There are several other packages that facilitate reading some forms of data. Geospatial data, in particular, requires a specialized library to interpret in R.

Data frames can be saved to disk using write.csv() and write.table(). This facilitates realistic data analysis pipelines entirely within R: read in data, wrangle, process, analyze, and export results.

A good segue into the next section is considering how we might easily find a file, or several types of files within the folders of our computer. We can use pattern matching or regular expression.

For example, consider that we have a very unorganized folder but we know that there are one or more .csv files. When listing files, you can add a pattern argument to the function so that the results only lists the type of files you are looking for:

```r
filename <- list.files("data/unorganized_folder", pattern = ".csv") 
df <- rea.csv(filename)
```

Read more about regular expression in the next section!

#### 4. Regular Expression

Characters, also known as "string" are a primary data type in any programming language. Regular expressions provides powerful string pattern matching capabilities using special characters.  This makes regular expressions invaluable for processing text-based data.

The first and easiest way to parse a character string is by splitting the string based on a known character.

```r
strsplit("Jose_Gonzalez", "_") # [1] "Jose"     "Gonzalez"
```

Other functions like `grep` and `grepl` search for matches to argument pattern within each element of a character and differ in the format of and amount of detail in the results. Whereas functions like `sub` and `gsub` perform replacement of the first and all matches respectively.

```r
grep("\\d", c("abc123", "def", "45def")) # match strings containing digits
grepl("[A-Z]", c("abc", "XYZ", "AbC")) # test for uppercase 
gsub("-", "_", c("text-data", "more-text")) # replace - with _
gsub("[[:punct:]]", "", c("Hi there!", "How are you?")) # remove punctuation
```
Regular expressions provide a powerful pattern matching syntax for text data. While you likely will not likely become an expert in regex, it is useful to know some if you want to automate some of the tedious tasks.

Some useful special characters for matching patterns include:

- \\d - digits
- \\s - whitespace
- . - any character
- [] - set/range of characters *(e.g. [0-9] are digits and [a-z] are lowercase letters)*
- ^ and $ - start and end of string

By combining these special characters with repetition qualifiers like * and +, we can create complex regex patterns to match all sorts of text-based formats. Here's a [cheatsheet](https://hypebright.nl/index.php/en/2020/05/25/ultimate-cheatsheet-for-regex-in-r-2/) if you want to know more.

#### 5. Basic Models

**Linear Models**

- Fit linear regression models with the lm() function
- Specify formula like y ~ x1 + x2
- summary() shows model coefficients, p-values
- fitted() and residuals() extract fitted values and residuals
- confint() gives confidence intervals for coefficients
- Should check model diagnostics like residual plots  

```r
fit <- lm(y ~ x1 + x2, data=mydata)
summary(fit) 
```
<><><>

R comes equipped out of the box with all of the standard statistical tests and models used in data analysis. Linear models can be fit using the `lm()` function and analyzed with `summary()`, `confint()`, and extractor functions for residuals, predictions, etc. 

We can fit simple linear regression models in R with the lm() function. It takes a formula as input describing the relationship between variables:

```r 
fit <- lm(y ~ x1 + x2, data=mydata)
summary(fit) # view model summary
confint(fit) # 95% CIs for coefficients 
```

Model residuals, predictions, diagnostics, and additional details can be extracted with extractor functions. We should check residuals for normality and homoscedasticity.

**significance**

Statistical tests like linear models produce p-values to test null hypotheses. P-values below 0.05 are often considered statistically significant:

```r
coef(summary(fit)) # view p-values for each coefficient
```

Get back t-statistic, degrees of freedom, p-value

However, with many comparisons, false positives become likely by chance (multiple comparisons problem). Adjustments like Bonferroni correction may be necessary.

- fitted() and residuals() extract fitted values and residuals

**ANOVA and T-tests** 

Hypothesis tests like t-tests, ANOVA, correlation tests are carried out with t.test(), aov(), and cor.test().

For group differences, R provides 1 and 2 sample t-tests, as well as ANOVA for factor-based group comparisons:

```r
t.test(x, y) # two sample t-test

aov(y ~ groups, data=mydata) # one-way ANOVA 
```

These tests make assumptions like normality and equal variance that should be checked. If violated, non-parametric tests may be better.

#### 4.4 Chi-squared and Correlation

Chi-squared tests allow analyzing relationships between categorical variables. Correlation like Pearson's r characterize numeric associations: 

```r
chisq.test(table(x, y)) 

cor.test(x, y) # correlation test 
```

Pearson for linear, Spearman for monotonic relationships

Scatterplots also help visualize correlations.


<><>



**t-tests** 

- Compare two groups with t.test()
- Specify the two samples with formula like x ~ y
- Get back t-statistic, degrees of freedom, p-value
- Paired (repeated measures) or unpaired (independent samples)

```r
t.test(x, y)
```

**ANOVA**

- Compare more than two groups with one-way ANOVA 
- aov(y ~ groups, data=mydata)
- Get F-statistic, p-value 
- Determine which groups differ using post-hoc tests like Tukey's 

```r
aov(y ~ groups, data=mydata)
```

**Correlation** 

- Test correlation between two numeric variables with cor.test()
- Get correlation coefficient and p-value
- Visualize with scatterplot and fitted line
- Pearson for linear, Spearman for monotonic relationships   

```r  
cor.test(x, y) 
```



<><>


#### Your Task 

Answer the following questions based on this week's content by writing an annotated R script.

#### 1. Data types and structures

Using the start *(Jan, 8)* and end *(April, 23)* dates of the spring 2024 semester, answer questions 1-3:

1. Find the number of days in the semester.

2. Extract all dates that fall on a Saturday and count the total number of Saturdays in the semester.

3. Round up and assume that we meet from 1-4PM (3 hrs) each day for this class. Use the dates of the semester and these details to calculate the number of hours you will spend in this class this semester. *(no need to consider holidays)*

Use regular expression and the following two vectors to respond to questions 4-5:

```r
# emails
emails <- c("w2u9q0g7z6@outlook.com", "m7n4w6f1a0@outlook.com", "withspaces@gmail.com",  
           "n7y4h5k0t6@gmail.com", "@invalid", "e2m5h7q1z0@outlook.com", 
           "invalid@", "invalid@", "invalid@",
           "g2b1j5u7o8@outlook.com", "s1c8k0t6e7@gmail.com", "f9m1a4z8h6@outlook.com",
           "e8c5f3v6k9@yahoo.com", "missingdotgmail.com", "invalid@email",
           "b6g3f0y4d2@gmail.com", "k0p8i9s6o3@outlook.com", "s8t7g2k9o5@yahoo.com", 
           "a9i7y1w5q0@gmail.com", "r2g9y4l5i6@yahoo.com", "b7d4a5u0j2@outlook.com",
           "l0a7z9m6r1@outlook.com", "c4v2a7w6r1@yahoo.com", "x1s3d8y5c6@yahoo.com",
           "@invalid", "@invalid", "@invalid",
           "@invalid", "invalid@", "e3k9x2m8t4@gmail.com",
           "invalidemail", "k2a3b7l1n8@outlook.com", "d0a6q9u2z4@yahoo.com",
           "i0f2z8c5x1@outlook.com", "q4m0l5o6j2@outlook.com", "v5h9a4f1q0@outlook.com",
           "@invalid", "f0z3h5d1v2@gmail.com", "g7k0u3a5s6@gmail.com",
           "@invalid", "invalid@", "@invalid", 
           "@invalid", "n9t0h3e6i5@outlook.com", "l2k1r6z0g5@outlook.com",
           "f9d7u5e1n8@outlook.com", "n9o4s8a6b0@yahoo.com", "@invalid",
           "s1b8q5i9z2@gmail.com", "z7v3n1o0f9@outlook.com")

# phone numbers
phones <- c("(525) 692-8788", "(748) 654-2046", "(865) 970-8066", "(310) 472-4003", 
           "(689) 943-4206", "920.680.4936", "323.171.5088", "265.687.3906", 
           "316.674.1293", "389.240.9468", "562 322 3503", "140 627 7741", 
           "530 215 9943", "189 705 9565", "415 873 7128", "133-854-8126", 
           "615-508-4357", "112-407-4979", "168-377-4229", "994-188-6602",
           "123-456-789", "abc-123-4567", "123 45 6789", "1a2b3c4d5e", 
           "12345", "67890", "1234", "6789", "12345678903", "12345678901")
```

4. With the emails vector, write a function to match only valid email addresses containing exactly one @ symbol and at least one period using regular expression.

5. Using the phones vector, write a regular expression pattern that could be used with grepl() or grep() to identify all valid 10-digit phone numbers in the phones vector, regardless of how they are formatted with parentheses, spaces, dots, etc. *(Tip: consider the format of different 10-digit numbers and account for optional non-digit characters, 3 digits, optional non-digits, 3 digits, optional non-digits, and 4 digits)* - advanced students should write a script to check to try and double check that their responses are valid.

Read in csv file provided in canvas. This file contains daily snow and meteorological data for 30 days from December, 24, 2023 to January, 22 2024.

After reading in the data, convert dates column to a date object and change the column names to more useful manageable names. Precipitation and Snow Water Equivalent units are in inches.

Snow water equivalent is the amount of water in the snowpack.

Respond to the following questions:

6. Is there a correlation between snow water equivalent and precipitation accumulation? How does the strength of this relationship change over the season?



7. How well does the snow water equivalent track with the median historical values? When does it tend to be above or below normal?



8. What is the relationship between air temperature and snow depth changes? How might rising temperatures impact total seasonal snowpack?

9. What are the typical day-to-day changes in snow water equivalent? How much new snow is required to increase the snowpack?

10. How often do major snowfall events (e.g. >6 inches) occur? Do these events contribute significantly to the total seasonal snowpack?

NEW

- how much snow was lost? over this time period

- using the equation swe = density * depth, determine the average and standard deviation density of the snowpack over this 30 day period. Express this number as a percentage and 

CHECK!


\

#### 2. Statistics

Use the `mtcars` dataset to understand basic relationships between variables. 

- The relationship between horsepower and fuel efficiency (mpg). A scatterplot can show if they are linearly related. Is engine power related to gas mileage?

- The distribution of fuel efficiency (mpg). A histogram can show the overall spread and central tendency. How is gas mileage distributed across cars?

- Differences in fuel efficiency by number of cylinders. A grouped boxplot can highlight differences in mpg by the variable cyl. Do cars with more cylinders have lower average mpg?

- The relationship between weight and fuel efficiency. A scatterplot can show if heavier cars tend to have lower mpg. Does weight impact gas mileage?

- Differences in fuel efficiency by transmission type. Group the mpg by am (transmission) and compare with a boxplot or densities plot. Do automatic or manual transmissions have better mpg?

- Horsepower differences by number of cylinders. Use a violin plot or boxplot to compare hp by cyl. Do cars with more cylinders tend to have higher horsepower?

- Weight differences by number of cylinders. Use a grouped boxplot to see how weight varies by cyl. Do cars with more cylinders tend to be heavier?




6. Calculate the correlation between mpg and wt. Is this correlation statistically significant at the 0.05 level?


7. Using the mtcars data, test whether the average mpg is different between cars with 4, 6, and 8 cylinders using ANOVA.



10. Write a function that simulates 10 coin flips and returns the number of heads. Run this simulation 1000 times and plot the distribution of heads.



<><><><><><><><><><><><><>


explore with the mtcars dataset:

- The relationship between horsepower and fuel efficiency (mpg). A scatterplot can show if they are linearly related. Is engine power related to gas mileage?

- The distribution of fuel efficiency (mpg). A histogram can show the overall spread and central tendency. How is gas mileage distributed across cars?

- Differences in fuel efficiency by number of cylinders. A grouped boxplot can highlight differences in mpg by the variable cyl. Do cars with more cylinders have lower average mpg?

- The relationship between weight and fuel efficiency. A scatterplot can show if heavier cars tend to have lower mpg. Does weight impact gas mileage?

- Differences in fuel efficiency by transmission type. Group the mpg by am (transmission) and compare with a boxplot or densities plot. Do automatic or manual transmissions have better mpg?

- Horsepower differences by number of cylinders. Use a violin plot or boxplot to compare hp by cyl. Do cars with more cylinders tend to have higher horsepower?

- Weight differences by number of cylinders. Use a grouped boxplot to see how weight varies by cyl. Do cars with more cylinders tend to be heavier?






## Data Formatting and Basic Statistics

### 1. Additional Data Formats

#### Time Formats

#### Dictionary-like Lists

#### Additional 

### 2. Regular Expression

### 3. Indexing and Editing DataFrames (base R)

### 4. Basic Models

#### 4.1 Statistical Linear Models

#### 4.2 Significance

#### 4.3 ANOVA and T-tests

#### 4.4 Chi-squared and Correlation

### 5. Reading and Writing Files & Filepaths

#### 5.1 Filepaths

#### 5.2 Loading and Saving Real Data

### Your Task

Answer the following 10 question based on this week's content by writing an annotated R script.
